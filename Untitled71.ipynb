{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "482f3604-4665-4435-a8c4-db092d7195ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load data from Excel file\n",
    "file_path = 'C:\\\\Users\\\\Admin\\\\OneDrive - MSFT\\\\Desktop\\\\jupyter\\\\Discs Dataset.xlsx'\n",
    "sheet_name_thickness = 'Thickness 1'  \n",
    "sheet_name_gap = 'Gap 2'\n",
    "sheet_name_height = 'Height'\n",
    "sheet_name_thickness_2 = 'Thickness 2'\n",
    "sheet_name_gap_1 = 'Gap 1'\n",
    "\n",
    "sheet_name_dia_3 = 'Diameter 3'  \n",
    "sheet_name_dia_2 = 'Diameter 2' \n",
    "\n",
    "\n",
    "data_thickness = pd.read_excel(file_path, sheet_name=sheet_name_thickness)\n",
    "data_thickness_2 = pd.read_excel(file_path, sheet_name=sheet_name_thickness_2)\n",
    "\n",
    "data_gap_1 = pd.read_excel(file_path, sheet_name=sheet_name_gap_1)\n",
    "data_gap = pd.read_excel(file_path, sheet_name=sheet_name_gap)\n",
    "\n",
    "data_height = pd.read_excel(file_path, sheet_name=sheet_name_height)\n",
    "\n",
    "data_dia_3 = pd.read_excel(file_path, sheet_name=sheet_name_dia_3)\n",
    "data_dia_2 = pd.read_excel(file_path, sheet_name=sheet_name_dia_2)\n",
    "\n",
    "\n",
    "all_x = []\n",
    "all_y = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b93d60b6-cca5-4cfd-ba5e-1bcf720353ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wavelength = data_thickness['Wavelength (nm)'].values\n",
    "\n",
    "# Define thickness values to interpolate\n",
    "thickness_values = [5, 10, 15, 20, 30, 50, 80, 100]\n",
    "\n",
    "constant_non_uniform_diameter = \"60nm80nm100nm120nm\"\n",
    "\n",
    "# Function to extract individual diameters and pad with NaN\n",
    "def extract_diameters(diameter_str, max_length=6):\n",
    "    diameters = []\n",
    "    for x in diameter_str.split('nm'):\n",
    "        if x:\n",
    "            diameters.append(int(x))\n",
    "    padded_diameters = diameters + [np.nan] * (max_length - len(diameters))\n",
    "    mask = [1] * len(diameters) + [0] * (max_length - len(diameters))\n",
    "    return padded_diameters, mask\n",
    "\n",
    "# Process each thickness value\n",
    "for thickness in thickness_values:\n",
    "    extension_ratio = data_thickness[f'{thickness} nm.2'].values  # Assuming the column name follows this pattern\n",
    "\n",
    "    # Interpolation\n",
    "    interpolation_function = interp1d(wavelength, extension_ratio, kind='linear', fill_value='extrapolate')\n",
    "\n",
    "    # Generate interpolated values for a range of wavelengths\n",
    "    wavelength_range = np.linspace(wavelength.min(), wavelength.max(), 2000)\n",
    "    interpolated_extension_ratios = interpolation_function(wavelength_range)\n",
    "\n",
    "    # Create input arrays with constants and varying inputs\n",
    "    batch_size = len(wavelength_range)\n",
    "    length_array = np.full((batch_size, 1), 50)\n",
    "    thickness_array = np.full((batch_size, 1), thickness)\n",
    "    gap_array = np.full((batch_size, 1), 5)  # Assume a constant gap for thickness data\n",
    "    wavelength_array = wavelength_range.reshape(batch_size, 1)\n",
    "\n",
    "    # Extract individual diameters and mask\n",
    "    diameter_array, mask_array = extract_diameters(constant_non_uniform_diameter)\n",
    "    diameter_array = np.tile(np.array(diameter_array), (batch_size, 1))\n",
    "    mask_array = np.tile(np.array(mask_array), (batch_size, 1))\n",
    "\n",
    "    # Create the input array\n",
    "    x = np.concatenate([length_array, thickness_array, gap_array, wavelength_array, diameter_array], axis=1)\n",
    "\n",
    "    # Append to storage\n",
    "    all_x.append(x)\n",
    "    all_y.append(interpolated_extension_ratios.reshape(batch_size, 1))\n",
    "\n",
    "# Combine all data\n",
    "x = np.vstack(all_x)\n",
    "y = np.vstack(all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b20d416a-9a7a-4868-9f07-8f8dde1930a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength = data_thickness_2['Wavelength (nm)'].values\n",
    "\n",
    "# Define thickness values to interpolate\n",
    "thickness_values = [2,4,6,8,10,15,20,25,30,40,50,70,80,100]\n",
    "\n",
    "# Constant non-uniform diameter string\n",
    "constant_non_uniform_diameter = \"40nm60nm80nm100nm120nm\"\n",
    "\n",
    "# Function to extract individual diameters and pad with NaN\n",
    "def extract_diameters(diameter_str, max_length=6):\n",
    "    diameters = []\n",
    "    for x in diameter_str.split('nm'):\n",
    "        if x:\n",
    "            diameters.append(int(x))\n",
    "    padded_diameters = diameters + [np.nan] * (max_length - len(diameters))\n",
    "    mask = [1] * len(diameters) + [0] * (max_length - len(diameters))\n",
    "    return padded_diameters, mask\n",
    "\n",
    "# Process each thickness value\n",
    "for thickness in thickness_values:\n",
    "    extension_ratio = data_thickness_2[f'{thickness}nm.2'].values  # Assuming the column name follows this pattern\n",
    "\n",
    "    # Interpolation\n",
    "    interpolation_function = interp1d(wavelength, extension_ratio, kind='linear', fill_value='extrapolate')\n",
    "\n",
    "    # Generate interpolated values for a range of wavelengths\n",
    "    wavelength_range = np.linspace(wavelength.min(), wavelength.max(), 2000)\n",
    "    interpolated_extension_ratios = interpolation_function(wavelength_range)\n",
    "\n",
    "    # Create input arrays with constants and varying inputs\n",
    "    batch_size = len(wavelength_range)\n",
    "    length_array = np.full((batch_size, 1), 50)\n",
    "    thickness_array = np.full((batch_size, 1), thickness)\n",
    "    gap_array = np.full((batch_size, 1), 5)  # Assume a constant gap for thickness data\n",
    "    wavelength_array = wavelength_range.reshape(batch_size, 1)\n",
    "\n",
    "    # Extract individual diameters and mask\n",
    "    diameter_array, mask_array = extract_diameters(constant_non_uniform_diameter)\n",
    "    diameter_array = np.tile(np.array(diameter_array), (batch_size, 1))\n",
    "    mask_array = np.tile(np.array(mask_array), (batch_size, 1))\n",
    "\n",
    "    # Create the input array\n",
    "    x = np.concatenate([length_array, thickness_array, gap_array, wavelength_array, diameter_array], axis=1)\n",
    "\n",
    "    # Append to storage\n",
    "    all_x.append(x)\n",
    "    all_y.append(interpolated_extension_ratios.reshape(batch_size, 1))\n",
    "\n",
    "# Combine all data\n",
    "x = np.vstack(all_x)\n",
    "y = np.vstack(all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1caba303-f04b-44c4-a063-20575481ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength = data_gap_1['Wavelength (nm)'].values\n",
    "\n",
    "gap_values = [4,5,6,8,10,15]\n",
    "\n",
    "constant_non_uniform_diameter = \"40nm60nm80nm100nm120nm\"\n",
    "\n",
    "# Process each gap value\n",
    "for gap in gap_values:\n",
    "    extension_ratio = data_gap_1[f'{gap} nm.2'].values  # Assuming the column name follows this pattern\n",
    "\n",
    "    # Interpolation\n",
    "    interpolation_function = interp1d(wavelength, extension_ratio, kind='linear', fill_value='extrapolate')\n",
    "\n",
    "    # Generate interpolated values for a range of wavelengths\n",
    "    wavelength_range = np.linspace(wavelength.min(), wavelength.max(), 2000)\n",
    "    interpolated_extension_ratios = interpolation_function(wavelength_range)\n",
    "\n",
    "    # Create input array with constants and varying inputs\n",
    "    batch_size = len(wavelength_range)\n",
    "    length_array = np.full((batch_size, 1), 50)\n",
    "    thickness_array = np.full((batch_size, 1), 50)\n",
    "    gap_array = np.full((batch_size, 1), gap)\n",
    "    wavelength_array = wavelength_range.reshape(batch_size, 1)\n",
    "\n",
    "    # Extract individual diameters and mask\n",
    "    diameter_array, mask_array = extract_diameters(constant_non_uniform_diameter)\n",
    "    diameter_array = np.tile(np.array(diameter_array), (batch_size, 1))\n",
    "    mask_array = np.tile(np.array(mask_array), (batch_size, 1))\n",
    "\n",
    "    # Create the input array\n",
    "    x = np.concatenate([length_array, thickness_array, gap_array, wavelength_array, diameter_array], axis=1)\n",
    "\n",
    "    # Append to storage\n",
    "    all_x.append(x)\n",
    "    all_y.append(interpolated_extension_ratios.reshape(batch_size, 1))\n",
    "\n",
    "# Combine all data\n",
    "x = np.vstack(all_x)\n",
    "y = np.vstack(all_y)\n",
    "\n",
    "# Shuffle the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f92e7678-c2ec-43c5-809f-d4ba59b28deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength = data_gap['Wavelength (nm)'].values\n",
    "\n",
    "gap_values = [5, 10, 15, 20]\n",
    "\n",
    "constant_non_uniform_diameter = \"60nm80nm100nm120nm\"\n",
    "\n",
    "# Process each gap value\n",
    "for gap in gap_values:\n",
    "    extension_ratio = data_gap[f'{gap} nm.2'].values  # Assuming the column name follows this pattern\n",
    "\n",
    "    # Interpolation\n",
    "    interpolation_function = interp1d(wavelength, extension_ratio, kind='linear', fill_value='extrapolate')\n",
    "\n",
    "    # Generate interpolated values for a range of wavelengths\n",
    "    wavelength_range = np.linspace(wavelength.min(), wavelength.max(), 2000)\n",
    "    interpolated_extension_ratios = interpolation_function(wavelength_range)\n",
    "\n",
    "    # Create input array with constants and varying inputs\n",
    "    batch_size = len(wavelength_range)\n",
    "    length_array = np.full((batch_size, 1), 50)\n",
    "    thickness_array = np.full((batch_size, 1), 50)\n",
    "    gap_array = np.full((batch_size, 1), gap)\n",
    "    wavelength_array = wavelength_range.reshape(batch_size, 1)\n",
    "\n",
    "    # Extract individual diameters and mask\n",
    "    diameter_array, mask_array = extract_diameters(constant_non_uniform_diameter)\n",
    "    diameter_array = np.tile(np.array(diameter_array), (batch_size, 1))\n",
    "    mask_array = np.tile(np.array(mask_array), (batch_size, 1))\n",
    "\n",
    "    # Create the input array\n",
    "    x = np.concatenate([length_array, thickness_array, gap_array, wavelength_array, diameter_array], axis=1)\n",
    "\n",
    "    # Append to storage\n",
    "    all_x.append(x)\n",
    "    all_y.append(interpolated_extension_ratios.reshape(batch_size, 1))\n",
    "\n",
    "# Combine all data\n",
    "x = np.vstack(all_x)\n",
    "y = np.vstack(all_y)\n",
    "\n",
    "# Shuffle the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db4fd234-5260-428c-9901-4b03aba1f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_values = [30, 50, 80, 100, 120, 150]\n",
    "\n",
    "constant_non_uniform_diameter = \"40nm60nm80nm100nm120nm\"\n",
    "\n",
    "for height in height_values:\n",
    "\n",
    "    extension_ratio = data_height[f'{height} nm.2'].values\n",
    "\n",
    "    interpolated_extension_ratios = interpolation_function(wavelength_range)\n",
    "    \n",
    "    batch_size = len(wavelength_range)\n",
    "    length_array = np.full((batch_size, 1), height)\n",
    "    thickness_array = np.full((batch_size, 1), 50)\n",
    "    gap_array = np.full((batch_size, 1), 5)  # Assume a constant gap for thickness data\n",
    "    wavelength_array = wavelength_range.reshape(batch_size, 1)\n",
    "\n",
    "        # Extract individual diameters and mask\n",
    "    diameter_array, mask_array = extract_diameters(constant_non_uniform_diameter)\n",
    "    diameter_array = np.tile(np.array(diameter_array), (batch_size, 1))\n",
    "    mask_array = np.tile(np.array(mask_array), (batch_size, 1))\n",
    "\n",
    "        # Create the input array\n",
    "    x = np.concatenate([length_array, thickness_array, gap_array, wavelength_array, diameter_array], axis=1)\n",
    "\n",
    "        # Append to storage\n",
    "    all_x.append(x)\n",
    "    all_y.append(interpolated_extension_ratios.reshape(batch_size, 1))\n",
    "\n",
    "# Combine all data\n",
    "x = np.vstack(all_x)\n",
    "y = np.vstack(all_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c259d26-bdff-432d-90a2-5881dcdc71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "non_uniform_diameter_values = [\n",
    "    \"80nm100nm\",\n",
    "    \"60nm80nm100nm\",\n",
    "    \"60nm80nm100nm120nm\",\n",
    "    \"40nm60nm80nm100nm120nm\",\n",
    "    \"40nm60nm80nm100nm120nm140nm\"\n",
    "]\n",
    "\n",
    "\n",
    "# Process each non-uniform diameter value\n",
    "for non_uniform_diameter in non_uniform_diameter_values:\n",
    "    # Extract individual diameters and mask\n",
    "    diameter_array, mask_array = extract_diameters(non_uniform_diameter)\n",
    "\n",
    "    # Interpolation for height data\n",
    "    extension_ratio = data_dia_3[f'{non_uniform_diameter}.2'].values\n",
    "    interpolation_function = interp1d(wavelength, extension_ratio, kind='linear', fill_value='extrapolate')\n",
    "    wavelength_range = np.linspace(wavelength.min(), wavelength.max(), 2000)\n",
    "    interpolated_extension_ratios = interpolation_function(wavelength_range)\n",
    "\n",
    "    # Create input arrays with constants and varying inputs\n",
    "    batch_size = len(wavelength_range)\n",
    "    length_array = np.full((batch_size, 1), 50)\n",
    "    thickness_array = np.full((batch_size, 1), 50)\n",
    "    gap_array = np.full((batch_size, 1), 5)\n",
    "    wavelength_array = wavelength_range.reshape(batch_size, 1)\n",
    "    diameter_array = np.tile(np.array(diameter_array), (batch_size, 1))\n",
    "    mask_array = np.tile(np.array(mask_array), (batch_size, 1))\n",
    "\n",
    "    # Create the input array\n",
    "    x = np.concatenate([length_array, thickness_array, gap_array, wavelength_array, diameter_array], axis=1)\n",
    "\n",
    "    # Append to storage\n",
    "    all_x.append(x)\n",
    "    all_y.append(interpolated_extension_ratios.reshape(batch_size, 1))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "x = np.vstack(all_x)\n",
    "y = np.vstack(all_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "669c844f-e0c8-425f-b0a4-0b3e85fd11b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "non_uniform_diameter_values = [\n",
    "    \"60nm80nm\",\n",
    "    \"80nm100nm\",\n",
    "    \"60nm80nm100nm\",\n",
    "    \"60nm80nm100nm120nm\",\n",
    "    \"40nm60nm80nm100nm120nm\",\n",
    "    \"40nm60nm80nm100nm120nm140nm\"\n",
    "]\n",
    "\n",
    "\n",
    "# Process each non-uniform diameter value\n",
    "for non_uniform_diameter in non_uniform_diameter_values:\n",
    "    # Extract individual diameters and mask\n",
    "    diameter_array, mask_array = extract_diameters(non_uniform_diameter)\n",
    "\n",
    "    # Interpolation for height data\n",
    "    extension_ratio = data_dia_2[f'{non_uniform_diameter}.2'].values\n",
    "    interpolation_function = interp1d(wavelength, extension_ratio, kind='linear', fill_value='extrapolate')\n",
    "    wavelength_range = np.linspace(wavelength.min(), wavelength.max(), 2000)\n",
    "    interpolated_extension_ratios = interpolation_function(wavelength_range)\n",
    "\n",
    "    # Create input arrays with constants and varying inputs\n",
    "    batch_size = len(wavelength_range)\n",
    "    length_array = np.full((batch_size, 1), 50)\n",
    "    thickness_array = np.full((batch_size, 1), 10)\n",
    "    gap_array = np.full((batch_size, 1), 5)\n",
    "    wavelength_array = wavelength_range.reshape(batch_size, 1)\n",
    "    diameter_array = np.tile(np.array(diameter_array), (batch_size, 1))\n",
    "    mask_array = np.tile(np.array(mask_array), (batch_size, 1))\n",
    "\n",
    "    # Create the input array\n",
    "    x = np.concatenate([length_array, thickness_array, gap_array, wavelength_array, diameter_array], axis=1)\n",
    "\n",
    "    # Append to storage\n",
    "    all_x.append(x)\n",
    "    all_y.append(interpolated_extension_ratios.reshape(batch_size, 1))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "x = np.vstack(all_x)\n",
    "y = np.vstack(all_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae047ba-5ec1-4a12-81ca-5accce4047be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all_x = np.vstack(all_x)\\n\\n# Convert to DataFrame\\n\\ncolumns=[\\'height\\',\\'thickness\\',\\'gap\\',\\'wavelength\\',\\'dia1\\',\\'dia2\\',\\'dia3\\',\\'dia4\\',\\'dia5\\',\\'dia6\\']\\n\\ndf = pd.DataFrame(all_x,columns=columns)\\n# Save to CSV\\ncsv_path = \\'C:\\\\Users\\\\Admin\\\\Desktop\\\\abcgd.csv\\'\\ndf.to_csv(csv_path, index=False,columns=columns)\\nprint(f\"Data saved to {csv_path}\")'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''all_x = np.vstack(all_x)\n",
    "\n",
    "# Convert to DataFrame\n",
    "\n",
    "columns=['height','thickness','gap','wavelength','dia1','dia2','dia3','dia4','dia5','dia6']\n",
    "\n",
    "df = pd.DataFrame(all_x,columns=columns)\n",
    "# Save to CSV\n",
    "csv_path = 'C:\\\\Users\\\\Admin\\\\Desktop\\\\abcgd.csv'\n",
    "df.to_csv(csv_path, index=False,columns=columns)\n",
    "print(f\"Data saved to {csv_path}\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a975f9d3-a0d3-4cd7-bab7-6c57747bc4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf_x_train = pd.DataFrame(x_train, columns=[f'feature_{i+1}' for i in range(x_train.shape[1])])\\ndf_y_train = pd.DataFrame(y_train, columns=['target'])\\n\\ndf_train = pd.concat([df_x_train, df_y_train], axis=1)\\n\\n\\ndf_train.to_csv('C:\\\\Users\\\\Admin\\\\Desktop\\\\train_data.csv', index=False)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[np.isnan(x)] = -1\n",
    "y[np.isnan(y)] = -1\n",
    "\n",
    "# Generate shuffled indices\n",
    "shuffled_indices = np.random.permutation(len(x))\n",
    "x_shuffled = x[shuffled_indices]\n",
    "y_shuffled = y[shuffled_indices]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_shuffled, y_shuffled, test_size=0.1, random_state=42)\n",
    "\n",
    "'''\n",
    "df_x_train = pd.DataFrame(x_train, columns=[f'feature_{i+1}' for i in range(x_train.shape[1])])\n",
    "df_y_train = pd.DataFrame(y_train, columns=['target'])\n",
    "\n",
    "df_train = pd.concat([df_x_train, df_y_train], axis=1)\n",
    "\n",
    "\n",
    "df_train.to_csv('C:\\\\Users\\\\Admin\\\\Desktop\\\\train_data.csv', index=False)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbf9a4be-bcee-4d67-b077-4b72f171bd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"data_combined = np.concatenate((x_train_scaled, y_train_scaled), axis=1)\\n\\n# Create a pandas DataFrame from the combined data\\ncolumns_x = [f'feature_{i}_scaled' for i in range(x_train_scaled.shape[1])]\\ncolumns_y = ['target_scaled']\\ncolumns_combined = columns_x + columns_y\\n\\ndf_combined = pd.DataFrame(data_combined, columns=columns_combined)\\n\\n# Save to CSV\\ncsv_file_path = 'C:\\\\Users\\\\Admin\\\\Desktop\\\\scaled_data_combined.csv'\\ndf_combined.to_csv(csv_file_path, index=False)\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "x_train_scaled = scaler_x.fit_transform(x_train)\n",
    "x_test_scaled = scaler_x.transform(x_test)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "'''data_combined = np.concatenate((x_train_scaled, y_train_scaled), axis=1)\n",
    "\n",
    "# Create a pandas DataFrame from the combined data\n",
    "columns_x = [f'feature_{i}_scaled' for i in range(x_train_scaled.shape[1])]\n",
    "columns_y = ['target_scaled']\n",
    "columns_combined = columns_x + columns_y\n",
    "\n",
    "df_combined = pd.DataFrame(data_combined, columns=columns_combined)\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = 'C:\\\\Users\\\\Admin\\\\Desktop\\\\scaled_data_combined.csv'\n",
    "df_combined.to_csv(csv_file_path, index=False)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d415fb-86f6-48aa-abd5-cc156e7ae210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 4ms/step - loss: 0.2030 - val_loss: 0.0699\n",
      "Epoch 2/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 0.0609 - val_loss: 0.0625\n",
      "Epoch 3/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.0542 - val_loss: 0.0470\n",
      "Epoch 4/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0463 - val_loss: 0.0429\n",
      "Epoch 5/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0449 - val_loss: 0.0421\n",
      "Epoch 6/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0436 - val_loss: 0.0420\n",
      "Epoch 7/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0408 - val_loss: 0.0474\n",
      "Epoch 8/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.0421 - val_loss: 0.0356\n",
      "Epoch 9/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 0.0402 - val_loss: 0.0399\n",
      "Epoch 10/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.0392 - val_loss: 0.0404\n",
      "Epoch 11/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.0395 - val_loss: 0.0351\n",
      "Epoch 12/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0398 - val_loss: 0.0341\n",
      "Epoch 13/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0370 - val_loss: 0.0417\n",
      "Epoch 14/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.0384 - val_loss: 0.0376\n",
      "Epoch 15/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 0.0376 - val_loss: 0.0346\n",
      "Epoch 16/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.0365 - val_loss: 0.0393\n",
      "Epoch 17/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.0355 - val_loss: 0.0338\n",
      "Epoch 18/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.0358 - val_loss: 0.0357\n",
      "Epoch 19/500\n",
      "\u001b[1m2757/2757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.0331 - val_loss: 0.0339\n",
      "Epoch 20/500\n",
      "\u001b[1m1779/2757\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0345"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_dim=x.shape[1], activation='relu'),  # First hidden layer with 32 neurons\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(32, activation='relu'),  \n",
    "    Dense(1, activation='linear')  \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(x_train_scaled, y_train_scaled, epochs=500, batch_size=32, validation_data=(x_test_scaled, y_test_scaled), verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model using Mean Absolute Error (MAE)\n",
    "predictions_scaled = model.predict(x_test_scaled)\n",
    "predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f'Mean Absolute Error (MAE): {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5095fed0-0989-4747-be42-adc50ffd0915",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions(thickness, height, gap, dia1, dia2, dia3, dia4, dia5, dia6):\n",
    "    # Generate a range of wavelengths\n",
    "    wavelength_range = np.linspace(400, 2500, 2500)\n",
    "    \n",
    "    # Create input array with constants and varying wavelength\n",
    "    batch_size = len(wavelength_range)\n",
    "    length_array = np.full((batch_size, 1), height)\n",
    "    thickness_array = np.full((batch_size, 1), thickness)\n",
    "    gap_array = np.full((batch_size, 1), gap)\n",
    "    wavelength_array = wavelength_range.reshape(batch_size, 1)\n",
    "    diameter_array = np.array([[dia1, dia2, dia3, dia4, dia5, dia6]] * batch_size)  # Replicate for each wavelength\n",
    "    \n",
    "    # Concatenate all inputs to form the final input batch\n",
    "    x_input = np.concatenate([length_array, thickness_array, gap_array, wavelength_array, diameter_array], axis=1)\n",
    "    \n",
    "    # Scale the inputs\n",
    "    x_input_scaled = scaler_x.transform(x_input)\n",
    "    \n",
    "    # Predict using the trained model\n",
    "    predictions_scaled = model.predict(x_input_scaled)\n",
    "    predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "    \n",
    "    return wavelength_range, predictions\n",
    "\n",
    "# Example usage:\n",
    "thickness_example = 60\n",
    "height_example = 20\n",
    "gap_example = 8\n",
    "dia1_example = 60\n",
    "dia2_example = 80\n",
    "dia3_example = -1\n",
    "dia4_example = -1\n",
    "dia5_example = -1\n",
    "dia6_example = -1\n",
    "\n",
    "wavelength_range_example, predictions_example = plot_predictions(thickness_example, height_example, gap_example,\n",
    "                                                                 dia1_example, dia2_example, dia3_example,\n",
    "                                                                 dia4_example, dia5_example, dia6_example)\n",
    "\n",
    "# Plotting the graph\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(wavelength_range_example, predictions_example, label='Predictions')\n",
    "plt.xlabel('Wavelength')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.title('Predictions vs Wavelength')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7db56c-1b84-4c89-8837-c4ea08fd93c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f85dc-80ce-43be-8ad7-226623a04ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
